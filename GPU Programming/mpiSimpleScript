#!/bin/bash              
#----------------------------------------------------
# Generic SLURM script -- MPI Hello World
#
# This script requests 2 nodes and all 16 cores/node
# for a total of 32 MPI tasks.
#----------------------------------------------------
#SBATCH -J myjob          # Job name
#SBATCH -o myjob.%j.out   # stdout; %j expands to jobid
#SBATCH -e myjob.%j.err   # stderr; skip to combine stdout and stderr
#SBATCH -p development    # queue
#SBATCH -N 2              # Number of nodes, not cores (16 cores/node)
#SBATCH -n 32             # Total number of MPI tasks (if omitted, n=N)
#SBATCH -t 00:30:00       # max time

#SBATCH --mail-user=username@tacc.utexas.edu
#SBATCH --mail-type=ALL

#SBATCH -A CMPS-5433-MWSU  # class project/account code;
                          # necessary if you have multiple project accounts

module restore system
module load fftw3         # Load any necessary modules (these are examples)
module list

ibrun ./main.exe        # Use ibrun only for MPI codes. Don't use mpirun or srun.
                        # Do not add "-n" or "-np" options here. SLURM infers the
                        # process count from the "-N" and "-n" directives above.